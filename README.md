# Awesome-MoE

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) 
![Stars](https://img.shields.io/github/stars/Oliver-FutureAI/Awesome-MoE)

Awesome list of Mixture-of-Experts (MoE) papers.

Kindly consider giving it a star if you find this list helpful. Thanks!

## News
- [2024-06-11] - Add papers of NeurIPS 2022, ICML 2022, ICML 2023
- [2024-06-10] - Add papers of NeurIPS 2023, ICCV 2023, CVPR 2023, ICLR 2023 
- [2024-05-24] - Add papers of ICLR 2024, CVPR 2024, ICCV 2023

[//]: # (## Table of Contents)


## Papers

Sort in descending chronological order, and conference event date.

| Venue               | Key Name | Title | Code |
|---------------------|----------|-------|------|
| 2024 CVPR           | TC-MoA | [Task-Customized Mixture of Adapters for General Image Fusion](https://arxiv.org/pdf/2403.12494) | [Link](https://github.com/YangSun22/TC-MoA) | 
| 2024 CVPR           | MLoRE | [Multi-Task Dense Prediction via Mixture of Low-Rank Experts](https://arxiv.org/pdf/2403.17749) | [Link](https://github.com/YuqiYang213/MLoRE) | 
| 2024 CVPR           | MoE-Adapters4CL | [Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters](https://arxiv.org/pdf/2403.11549) | [Link](https://github.com/JiazuoYu/MoE-Adapters4CL) | 
| 2024 CVPR           | Omni-SMoLA | [Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts](https://arxiv.org/pdf/2312.00968) | N/A | 
| 2024 ICLR           | LLMCarbon | [LLMCarbon: Modeling the End-to-End Carbon Footprint of Large Language Models](https://openreview.net/pdf?id=aIok3ZD9to) | [Link](https://github.com/SotaroKaneda/MLCarbon) | 
| 2024 ICLR           | Soft MoE | [From Sparse to Soft Mixtures of Experts](https://openreview.net/pdf?id=jxpsAj7ltE) | [Link](https://github.com/bwconrad/soft-moe) | 
| 2024 ICLR           | MC-SMoE | [Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy](https://openreview.net/pdf?id=eFWG9Cy3WK) | [Link](https://github.com/UNITES-Lab/MC-SMoE) | 
| 2024 ICLR           | Mowst | [Mixture of Weak and Strong Experts on Graphs](https://openreview.net/attachment?id=wYvuY60SdD&name=pdf) | [Link](https://github.com/facebookresearch/mowst-gnn) | 
| 2024 ICLR           | MoV | [Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning](https://openreview.net/pdf?id=EvDeiLv7qc) | [Link](https://github.com/for-ai/parameter-efficient-moe) | 
| 2024 ICLR           | PI-HC-MoE | [Scaling physics-informed hard constraints with mixture-of-experts](https://openreview.net/pdf?id=u3dX2CEIZb) | [Link](https://github.com/ASK-Berkeley/physics-NNs-hard-constraints) | 
| 2024 ICLR           | MoLE | [Mixture of LoRA Experts](https://openreview.net/pdf?id=uWvKBCYh4S) | [Link](https://github.com/yushuiwx/MoLE) | 
| 2024 ICLR           | TESTAM | [TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts](https://openreview.net/pdf?id=N0nTk5BSvO) | [Link](https://github.com/HyunWookL/TESTAM) | 
| 2024 ICLR           | MOORE | [Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts](https://openreview.net/pdf?id=aZH1dM3GOX) | [Link](https://github.com/AhmedMagdyHendawy/MOORE) | 
| 2024 ICLR           | HSQ | [Hybrid Sharing for Multi-Label Image Classification](https://openreview.net/pdf?id=yVJd8lKyVX) | [Link](https://github.com/zihao-yin/HSQ) | 
| 2024 ICLR           | FLAN-MOE | [Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models](https://openreview.net/pdf?id=6mLjDwYte5) | N/A | 
| 2024 ICLR           | Lingual-SMoE | [Sparse MoE with Language Guided Routing for Multilingual Machine Translation](https://openreview.net/pdf?id=ySS7hH1smL) | [Link](https://github.com/UNITES-Lab/Lingual-SMoE) | 
| 2023 NeurIPS           | ShiftAddViT | [ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer](https://papers.neurips.cc/paper_files/paper/2023/file/69c49f75ca31620f1f0d38093d9f3d9b-Paper-Conference.pdf) | [Link](https://github.com/GATECH-EIC/ShiftAddViT) | 
| 2023 NeurIPS           | RAPHAEL | [RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths](https://papers.neurips.cc/paper_files/paper/2023/file/821655c7dc4836838cd8524d07f9d6fd-Paper-Conference.pdf) | N/A | 
| 2023 NeurIPS           | DAMEX | [DAMEX: Dataset-aware Mixture-of-Experts for visual understanding of mixture-of-datasets](https://arxiv.org/pdf/2311.04894) | [Link](https://github.com/jinga-lala/DAMEX) | 
| 2023 NeurIPS           | MoE-IMP | [Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception](https://arxiv.org/pdf/2305.06324) | N/A | 
| 2023 ICCV           | AdaMV-MoE | [AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_AdaMV-MoE_Adaptive_Multi-Task_Vision_Mixture-of-Experts_ICCV_2023_paper.pdf) | [Link](https://github.com/google-research/google-research/tree/master/moe_mtl) | 
| 2023 ICCV           | MoE-Fusion | [Multi-Modal Gated Mixture of Local-to-Global Experts for Dynamic Image Fusion](https://arxiv.org/pdf/2302.01392) | [Link](https://github.com/SunYM2020/MoE-Fusion) | 
| 2023 ICCV           | PnD | [Partition-and-Debias: Agnostic Biases Mitigation via A Mixture of Biases-Specific Experts](https://arxiv.org/pdf/2308.10005) | [Link](https://github.com/Jiaxuan-Li/PnD) | 
| 2023 ICCV           | TaskExpert | [TaskExpert: Dynamically Assembling Multi-Task Representations with Memorial Mixture-of-Experts](https://arxiv.org/pdf/2307.15324) | [Link](https://github.com/prismformore/Multi-Task-Transformer) | 
| 2023 ICCV           | GNT-MOVE | [Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts](https://arxiv.org/pdf/2308.11793) | [Link](https://github.com/VITA-Group/GNT-MOVE) | 
| 2023 ICCV           | ADVMoE | [Robust Mixture-of-Expert Training for Convolutional Neural Networks](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Robust_Mixture-of-Expert_Training_for_Convolutional_Neural_Networks_ICCV_2023_paper.pdf) | [Link](https://github.com/OPTML-Group/Robust-MoE-CNN) | 
| 2023 ICML           | pMoE | [Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolutional Neural Networks](https://proceedings.mlr.press/v202/chowdhury23a/chowdhury23a.pdf) | [Link](https://github.com/nowazrabbani/pMoE_CNN) | 
| 2023 CVPR           | ERNIE-ViLG 2.0 | [ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model With Knowledge-Enhanced Mixture-of-Denoising-Experts](https://arxiv.org/pdf/2210.15257) | N/A | 
| 2023 CVPR           | Mod-Squad | [Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners](https://arxiv.org/pdf/2212.08066) | [Link](https://github.com/UMass-Foundation-Model/Mod-Squad) | 
| 2023 ICLR           | GMoE | [Sparse Mixture-of-Experts are Domain Generalizable Learners](https://arxiv.org/pdf/2206.04046) | [Link](https://github.com/Luodian/Generalizable-Mixture-of-Experts) | 
| 2023 ICLR           | SMoE-Dropout | [Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers](https://arxiv.org/pdf/2303.01610) | [Link](https://github.com/VITA-Group/Random-MoE-as-Dropout) | 
| 2023 ICLR           | KiC | [Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models](https://arxiv.org/pdf/2210.16433) | N/A | 
| 2023 ICLR           | MoCE | [Task-customized Masked Autoencoder via Mixture of Cluster-conditional Experts](https://arxiv.org/pdf/2402.05382) | N/A | 
| 2023 ICLR           | SCoMoE | [SCoMoE: Efficient Mixtures of Experts with Structured Communication](https://openreview.net/pdf?id=s-c96mSU0u5) | [Link](https://github.com/ZhiYuanZeng/fairseq-moe) | 
| 2023 ICLR           | Switch-NeRF | [Switch-NeRF: Learning Scene Decomposition with Mixture of Experts for Large-scale Neural Radiance Fields](https://openreview.net/pdf?id=PQ2zoIZqvm) | [Link](https://github.com/MiZhenxing/Switch-NeRF) | 
| 2022 NeurIPS           | N/A | [Towards Understanding the Mixture-of-Experts Layer in Deep Learning](https://papers.nips.cc/paper_files/paper/2022/file/91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf) | [Link](https://github.com/uclaml/MoE) | 
| 2022 NeurIPS           | Uni-Perceiver-MoE | [Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs](https://papers.nips.cc/paper_files/paper/2022/file/11fc8c98b46d4cbdfe8157267228f7d7-Paper-Conference.pdf) | [Link](https://github.com/fundamentalvision/Uni-Perceiver) | 
| 2022 NeurIPS           | LIMoE | [Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts](https://papers.nips.cc/paper_files/paper/2022/file/3e67e84abf900bb2c7cbd5759bfce62d-Paper-Conference.pdf) | N/A | 
| 2022 NeurIPS           | TA-MoE | [TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training](https://papers.nips.cc/paper_files/paper/2022/file/8b465dd58ac50e1b0b22894fd581f62f-Paper-Conference.pdf) | N/A | 
| 2022 NeurIPS           | Meta-DMoE | [Meta-DMoE: Adapting to Domain Shift by Meta-Distillation from Mixture-of-Experts](https://papers.nips.cc/paper_files/paper/2022/file/8bd4f1dbc7a70c6b80ce81b8b4fdc0b2-Paper-Conference.pdf) | [Link](https://github.com/n3il666/Meta-DMoE) | 
| 2022 NeurIPS           | M³ViT | [M³ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design](https://arxiv.org/pdf/2210.14793) | [Link](https://github.com/VITA-Group/M3ViT) | 
| 2022 NeurIPS           | SMoE | [Spatial Mixture-of-Experts](https://papers.nips.cc/paper_files/paper/2022/file/4c5e2bcbf21bdf40d75fddad0bd43dc9-Paper-Conference.pdf) | [Link](https://github.com/spcl/smoe) | 
| 2022 NeurIPS           | MoE-NPs | [Learning Expressive Meta-Representations with Mixture of Expert Neural Processes](https://papers.nips.cc/paper_files/paper/2022/file/a815fe7cad6af20a6c118f2072a881d2-Paper-Conference.pdf) | N/A | 
| 2022 NeurIPS           | VLMo | [VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts](https://papers.nips.cc/paper_files/paper/2022/file/d46662aa53e78a62afd980a29e0c37ed-Paper-Conference.pdf) | [Link](https://github.com/microsoft/unilm/tree/master/vlmo) | 
| 2022 NeurIPS           | X-MoE | [On the Representation Collapse of Sparse Mixture of Experts](https://papers.nips.cc/paper_files/paper/2022/file/df4f371f1f89ec8ba5014b3310578048-Paper-Conference.pdf) | N/A | 
| 2022 ICML           | NID | [Neural Implicit Dictionary Learning via Mixture-of-Expert Training](https://proceedings.mlr.press/v162/wang22d/wang22d.pdf) | [Link](https://github.com/VITA-Group/Neural-Implicit-Dict) | 
| 2022 ICML           | GLaM | [GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://proceedings.mlr.press/v162/du22c/du22c.pdf) | N/A | 
| 2022 ICML           | DeepSpeed-MoE | [DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale](https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf) | [Link](https://github.com/microsoft/DeepSpeed) | 

* Note: there are no papers of MoE topic in 2022 ECCV, 2022 CVPR and 2022 ICLR.

[//]: # (## Tutorials)


[//]: # (## Citation)


## Other

Please do not distribute this list without permission. Thank you.
