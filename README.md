# Awesome-MoE

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) 
![Stars](https://img.shields.io/github/stars/Oliver-FutureAI/Awesome-MoE)
[![Visits Badge](https://badges.pufler.dev/visits/Oliver-FutureAI/Awesome-MoE)](https://badges.pufler.dev/visits/Oliver-FutureAI/Awesome-MoE)

Awesome list of Mixture-of-Experts (MoE) papers.

Kindly consider giving a star if you find this list helpful. Thanks!

## News
- Add papers of NeurIPS 2023, ICCV 2023, CVPR 2023, ICLR 2023, NeurIPS 2022
- Add papers of ICLR 2024, CVPR 2024, ICCV 2023

[//]: # (## Table of Contents)


## Papers

Sort in descending chronological order.

| Venue               | Key Name | Title | Code |
|---------------------|----------|-------|------|
| 2024 CVPR           | TC-MoA | [Task-Customized Mixture of Adapters for General Image Fusion](https://arxiv.org/pdf/2403.12494) | [Link](https://github.com/YangSun22/TC-MoA) | 
| 2024 CVPR           | MLoRE | [Multi-Task Dense Prediction via Mixture of Low-Rank Experts](https://arxiv.org/pdf/2403.17749) | [Link](https://github.com/YuqiYang213/MLoRE) | 
| 2024 CVPR           | MoE-Adapters4CL | [Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters](https://arxiv.org/pdf/2403.11549) | [Link](https://github.com/JiazuoYu/MoE-Adapters4CL) | 
| 2024 CVPR           | Omni-SMoLA | [Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts](https://arxiv.org/pdf/2312.00968) | N/A | 
| 2024 ICLR           | LLMCarbon | [LLMCarbon: Modeling the End-to-End Carbon Footprint of Large Language Models](https://openreview.net/pdf?id=aIok3ZD9to) | [Link](https://github.com/SotaroKaneda/MLCarbon) | 
| 2024 ICLR           | Soft MoE | [From Sparse to Soft Mixtures of Experts](https://openreview.net/pdf?id=jxpsAj7ltE) | [Link](https://github.com/bwconrad/soft-moe) | 
| 2024 ICLR           | MC-SMoE | [Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy](https://openreview.net/pdf?id=eFWG9Cy3WK) | [Link](https://github.com/UNITES-Lab/MC-SMoE) | 
| 2024 ICLR           | Mowst | [Mixture of Weak and Strong Experts on Graphs](https://openreview.net/attachment?id=wYvuY60SdD&name=pdf) | [Link](https://github.com/facebookresearch/mowst-gnn) | 
| 2024 ICLR           | MoV | [Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning](https://openreview.net/pdf?id=EvDeiLv7qc) | [Link](https://github.com/for-ai/parameter-efficient-moe) | 
| 2024 ICLR           | PI-HC-MoE | [Scaling physics-informed hard constraints with mixture-of-experts](https://openreview.net/pdf?id=u3dX2CEIZb) | [Link](https://github.com/ASK-Berkeley/physics-NNs-hard-constraints) | 
| 2024 ICLR           | MoLE | [Mixture of LoRA Experts](https://openreview.net/pdf?id=uWvKBCYh4S) | [Link](https://github.com/yushuiwx/MoLE) | 
| 2024 ICLR           | TESTAM | [TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts](https://openreview.net/pdf?id=N0nTk5BSvO) | [Link](https://github.com/HyunWookL/TESTAM) | 
| 2024 ICLR           | MOORE | [Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts](https://openreview.net/pdf?id=aZH1dM3GOX) | [Link](https://github.com/AhmedMagdyHendawy/MOORE) | 
| 2024 ICLR           | HSQ | [Hybrid Sharing for Multi-Label Image Classification](https://openreview.net/pdf?id=yVJd8lKyVX) | [Link](https://github.com/zihao-yin/HSQ) | 
| 2024 ICLR           | FLAN-MOE | [Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models](https://openreview.net/pdf?id=6mLjDwYte5) | N/A | 
| 2024 ICLR           | Lingual-SMoE | [Sparse MoE with Language Guided Routing for Multilingual Machine Translation](https://openreview.net/pdf?id=ySS7hH1smL) | [Link](https://github.com/UNITES-Lab/Lingual-SMoE) | 
| 2023 NeurIPS           | ShiftAddViT | [ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer](https://papers.neurips.cc/paper_files/paper/2023/file/69c49f75ca31620f1f0d38093d9f3d9b-Paper-Conference.pdf) | [Link](https://github.com/GATECH-EIC/ShiftAddViT) | 
| 2023 NeurIPS           | RAPHAEL | [RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths](https://papers.neurips.cc/paper_files/paper/2023/file/821655c7dc4836838cd8524d07f9d6fd-Paper-Conference.pdf) | N/A | 
| 2023 NeurIPS           | DAMEX | [DAMEX: Dataset-aware Mixture-of-Experts for visual understanding of mixture-of-datasets](https://arxiv.org/pdf/2311.04894) | [Link](https://github.com/jinga-lala/DAMEX) | 
| 2023 NeurIPS           | MoE-IMP | [Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception](https://arxiv.org/pdf/2305.06324) | N/A | 
| 2023 ICCV           | AdaMV-MoE | [AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_AdaMV-MoE_Adaptive_Multi-Task_Vision_Mixture-of-Experts_ICCV_2023_paper.pdf) | [Link](https://github.com/google-research/google-research/tree/master/moe_mtl) | 
| 2023 ICCV           | MoE-Fusion | [Multi-Modal Gated Mixture of Local-to-Global Experts for Dynamic Image Fusion](https://arxiv.org/pdf/2302.01392) | [Link](https://github.com/SunYM2020/MoE-Fusion) | 
| 2023 ICCV           | PnD | [Partition-and-Debias: Agnostic Biases Mitigation via A Mixture of Biases-Specific Experts](https://arxiv.org/pdf/2308.10005) | [Link](https://github.com/Jiaxuan-Li/PnD) | 
| 2023 ICCV           | TaskExpert | [TaskExpert: Dynamically Assembling Multi-Task Representations with Memorial Mixture-of-Experts](https://arxiv.org/pdf/2307.15324) | [Link](https://github.com/prismformore/Multi-Task-Transformer) | 
| 2023 ICCV           | GNT-MOVE | [Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts](https://arxiv.org/pdf/2308.11793) | [Link](https://github.com/VITA-Group/GNT-MOVE) | 
| 2023 ICCV           | ADVMoE | [Robust Mixture-of-Expert Training for Convolutional Neural Networks](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Robust_Mixture-of-Expert_Training_for_Convolutional_Neural_Networks_ICCV_2023_paper.pdf) | [Link](https://github.com/OPTML-Group/Robust-MoE-CNN) | 
| 2023 CVPR           | ERNIE-ViLG 2.0 | [ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model With Knowledge-Enhanced Mixture-of-Denoising-Experts](https://arxiv.org/pdf/2210.15257) | N/A | 
| 2023 CVPR           | Mod-Squad | [Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners](https://arxiv.org/pdf/2212.08066) | [Link](https://github.com/UMass-Foundation-Model/Mod-Squad) | 
| 2023 ICLR           | N/A | [A Mixture-of-Expert Approach to RL-based Dialogue Management](https://arxiv.org/pdf/2206.00059) | N/A | 
| 2023 ICLR           | GMoE | [Sparse Mixture-of-Experts are Domain Generalizable Learners](https://arxiv.org/pdf/2206.04046) | [Link](https://github.com/Luodian/Generalizable-Mixture-of-Experts) | 
| 2023 ICLR           | SMoE-Dropout | [Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers](https://arxiv.org/pdf/2303.01610) | [Link](https://github.com/VITA-Group/Random-MoE-as-Dropout) | 
| 2023 ICLR           | KiC | [Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models](https://arxiv.org/pdf/2210.16433) | N/A | 
| 2023 ICLR           | MoCE | [Task-customized Masked Autoencoder via Mixture of Cluster-conditional Experts](https://arxiv.org/pdf/2402.05382) | N/A | 
| 2023 ICLR           | SCoMoE | [SCoMoE: Efficient Mixtures of Experts with Structured Communication](https://openreview.net/pdf?id=s-c96mSU0u5) | [Link](https://github.com/ZhiYuanZeng/fairseq-moe) | 
| 2023 ICLR           | Switch-NeRF | [Switch-NeRF: Learning Scene Decomposition with Mixture of Experts for Large-scale Neural Radiance Fields](https://openreview.net/pdf?id=PQ2zoIZqvm) | [Link](https://github.com/MiZhenxing/Switch-NeRF) | 
| 2022 NeurIPS           | M³ViT | [M³ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design](https://arxiv.org/pdf/2210.14793) | [Link](https://github.com/VITA-Group/M3ViT) | 


[//]: # (## Tutorials)


[//]: # (## Citation)


## Other

Please do not distribute this list without permission. Thank you.
