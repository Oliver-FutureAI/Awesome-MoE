# Awesome-MoE
Awesome list of Mixture-of-Experts (MoE)

## News

- Add papers of ICLR 2024, CVPR 2024

## Table of Contents

## Papers

| Venue               | Key Name | Title | Code |
|---------------------| --- | --- | --- |
| 2024 ICLR           | Lingual-SMoE | [Sparse MoE with Language Guided Routing for Multilingual Machine Translation](https://openreview.net/pdf?id=ySS7hH1smL) | [Link](https://github.com/UNITES-Lab/Lingual-SMoE) |
| 2024 ICLR           | FLAN-MOE | [Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models](https://openreview.net/pdf?id=6mLjDwYte5) | N/A |
| 2024 ICLR           | HSQ | [Hybrid Sharing for Multi-Label Image Classification](https://openreview.net/pdf?id=yVJd8lKyVX) | [Link](https://github.com/zihao-yin/HSQ) |
| 2024 ICLR           | MOORE | [Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts](https://openreview.net/pdf?id=aZH1dM3GOX) | [Link](https://github.com/AhmedMagdyHendawy/MOORE) |
| 2024 ICLR           | TESTAM | [TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts](https://openreview.net/pdf?id=N0nTk5BSvO) | [Link](https://github.com/HyunWookL/TESTAM) |
| 2024 ICLR           | MoLE | [Mixture of LoRA Experts](https://openreview.net/pdf?id=uWvKBCYh4S) | [Link](https://github.com/yushuiwx/MoLE) |
| 2024 ICLR           | PI-HC-MoE | [Scaling physics-informed hard constraints with mixture-of-experts](https://openreview.net/pdf?id=u3dX2CEIZb) | [Link](https://github.com/ASK-Berkeley/physics-NNs-hard-constraints) |
| 2024 ICLR           | N/A | [Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts](https://openreview.net/pdf?id=jvtmdK69KQ) | N/A |
| 2024 ICLR           | MoV | [Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning](https://openreview.net/pdf?id=EvDeiLv7qc) | [Link](https://github.com/for-ai/parameter-efficient-moe) |
| 2024 ICLR           | Mowst | [Mixture of Weak and Strong Experts on Graphs](https://openreview.net/attachment?id=wYvuY60SdD&name=pdf) | [Link](https://github.com/facebookresearch/mowst-gnn) |
| 2024 ICLR Spotlight | MC-SMoE | [Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy](https://openreview.net/pdf?id=eFWG9Cy3WK) | [Link](https://github.com/UNITES-Lab/MC-SMoE) |
| 2024 ICLR Spotlight | Soft MoE | [From Sparse to Soft Mixtures of Experts](https://openreview.net/pdf?id=jxpsAj7ltE) | [Link](https://github.com/bwconrad/soft-moe) |
| 2024 ICLR Oral      | LLMCarbon | [LLMCarbon: Modeling the End-to-End Carbon Footprint of Large Language Models](https://openreview.net/pdf?id=aIok3ZD9to) | [Link](https://github.com/SotaroKaneda/MLCarbon) |
| 2024 CVPR           | Omni-SMoLA | [Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts](https://arxiv.org/pdf/2312.00968) | N/A |
| 2024 CVPR           | MoE-Adapters4CL | [Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters](https://arxiv.org/pdf/2403.11549) | [Link](https://github.com/JiazuoYu/MoE-Adapters4CL) |
| 2024 CVPR           | MLoRE | [BMulti-Task Dense Prediction via Mixture of Low-Rank Experts](https://arxiv.org/pdf/2403.17749) | [Link](https://github.com/YuqiYang213/MLoRE) |
| 2024 CVPR           | TC-MoA | [Task-Customized Mixture of Adapters for General Image Fusion](https://arxiv.org/pdf/2403.12494) | [Link](https://github.com/YangSun22/TC-MoA) |


## Tutorials


## Citation

## Other

Please do not distribute this list without permission. Thank you.
