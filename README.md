# Awesome-MoE
Awesome list of Mixture-of-Experts (MoE)

## News

- Add papers of ICLR 2024, CVPR 2024, ICCV 2023

## Table of Contents

## Papers

| Venue               | Key Name | Title | Code |
|---------------------| --- | --- | --- |
| 2024 ICLR           | Lingual-SMoE | [Sparse MoE with Language Guided Routing for Multilingual Machine Translation](https://openreview.net/pdf?id=ySS7hH1smL) | [Link](https://github.com/UNITES-Lab/Lingual-SMoE) |
| 2024 ICLR           | FLAN-MOE | [Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models](https://openreview.net/pdf?id=6mLjDwYte5) | N/A |
| 2024 ICLR           | HSQ | [Hybrid Sharing for Multi-Label Image Classification](https://openreview.net/pdf?id=yVJd8lKyVX) | [Link](https://github.com/zihao-yin/HSQ) |
| 2024 ICLR           | MOORE | [Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts](https://openreview.net/pdf?id=aZH1dM3GOX) | [Link](https://github.com/AhmedMagdyHendawy/MOORE) |
| 2024 ICLR           | TESTAM | [TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts](https://openreview.net/pdf?id=N0nTk5BSvO) | [Link](https://github.com/HyunWookL/TESTAM) |
| 2024 ICLR           | MoLE | [Mixture of LoRA Experts](https://openreview.net/pdf?id=uWvKBCYh4S) | [Link](https://github.com/yushuiwx/MoLE) |
| 2024 ICLR           | PI-HC-MoE | [Scaling physics-informed hard constraints with mixture-of-experts](https://openreview.net/pdf?id=u3dX2CEIZb) | [Link](https://github.com/ASK-Berkeley/physics-NNs-hard-constraints) |
| 2024 ICLR           | N/A | [Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts](https://openreview.net/pdf?id=jvtmdK69KQ) | N/A |
| 2024 ICLR           | MoV | [Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning](https://openreview.net/pdf?id=EvDeiLv7qc) | [Link](https://github.com/for-ai/parameter-efficient-moe) |
| 2024 ICLR           | Mowst | [Mixture of Weak and Strong Experts on Graphs](https://openreview.net/attachment?id=wYvuY60SdD&name=pdf) | [Link](https://github.com/facebookresearch/mowst-gnn) |
| 2024 ICLR Spotlight | MC-SMoE | [Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy](https://openreview.net/pdf?id=eFWG9Cy3WK) | [Link](https://github.com/UNITES-Lab/MC-SMoE) |
| 2024 ICLR Spotlight | Soft MoE | [From Sparse to Soft Mixtures of Experts](https://openreview.net/pdf?id=jxpsAj7ltE) | [Link](https://github.com/bwconrad/soft-moe) |
| 2024 ICLR Oral      | LLMCarbon | [LLMCarbon: Modeling the End-to-End Carbon Footprint of Large Language Models](https://openreview.net/pdf?id=aIok3ZD9to) | [Link](https://github.com/SotaroKaneda/MLCarbon) |
| 2024 CVPR           | Omni-SMoLA | [Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts](https://arxiv.org/pdf/2312.00968) | N/A |
| 2024 CVPR           | MoE-Adapters4CL | [Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters](https://arxiv.org/pdf/2403.11549) | [Link](https://github.com/JiazuoYu/MoE-Adapters4CL) |
| 2024 CVPR           | MLoRE | [BMulti-Task Dense Prediction via Mixture of Low-Rank Experts](https://arxiv.org/pdf/2403.17749) | [Link](https://github.com/YuqiYang213/MLoRE) |
| 2024 CVPR           | TC-MoA | [Task-Customized Mixture of Adapters for General Image Fusion](https://arxiv.org/pdf/2403.12494) | [Link](https://github.com/YangSun22/TC-MoA) |
| 2023 ICCV           | ADVMoE | [Robust Mixture-of-Expert Training for Convolutional Neural Networks](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Robust_Mixture-of-Expert_Training_for_Convolutional_Neural_Networks_ICCV_2023_paper.pdf) | [Link](https://github.com/OPTML-Group/Robust-MoE-CNN) |
| 2023 ICCV           | GNT-MOVE | [Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts](https://arxiv.org/pdf/2308.11793) | [Link](https://github.com/VITA-Group/GNT-MOVE) |
| 2023 ICCV           | TaskExpert | [TaskExpert: Dynamically Assembling Multi-Task Representations with Memorial Mixture-of-Experts](https://arxiv.org/pdf/2307.15324) | [Link](https://github.com/prismformore/Multi-Task-Transformer) |
| 2023 ICCV           | PnD | [Partition-and-Debias: Agnostic Biases Mitigation via A Mixture of Biases-Specific Experts](https://arxiv.org/pdf/2308.10005) | [Link](https://github.com/Jiaxuan-Li/PnD) |
| 2023 ICCV           | MoE-Fusion | [Multi-Modal Gated Mixture of Local-to-Global Experts for Dynamic Image Fusion](https://arxiv.org/pdf/2302.01392) | [Link](https://github.com/SunYM2020/MoE-Fusion) |
| 2023 ICCV           | AdaMV-MoE | [AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_AdaMV-MoE_Adaptive_Multi-Task_Vision_Mixture-of-Experts_ICCV_2023_paper.pdf) | [Link](https://github.com/google-research/google-research/tree/master/moe_mtl) |




[//]: # (| 2022 NeurIPS        | M³ViT | [M³ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design]&#40;https://arxiv.org/pdf/2210.14793&#41; | [Link]&#40;https://github.com/VITA-Group/M3ViT&#41; |)


[//]: # (## Tutorials)


[//]: # (## Citation)

## Other

Please do not distribute this list without permission. Thank you.
